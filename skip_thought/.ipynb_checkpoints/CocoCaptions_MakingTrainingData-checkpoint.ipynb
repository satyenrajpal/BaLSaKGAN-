{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.80s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "data = dset.CocoCaptions(root = '/home/ubuntu/val2014/',\n",
    "                        annFile = '/home/ubuntu/coco/annotations/captions_val2014.json',\n",
    "                        transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/vocab_file.pkl', 'rb') as handle:\n",
    "    indx_to_word_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_indx_dict = dict (zip(indx_to_word_dict.values(),indx_to_word_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skateboarder is putting on a show using the picnic table as his stage.\n",
      "A skateboarder pulling tricks on top of a picnic table.\n",
      "A man riding on a skateboard on top of a table.\n",
      "A skate boarder doing a trick on a picnic table.\n",
      "A person is riding a skateboard on a picnic table with a crowd watching.\n",
      "set([u'picnic', u'crowd', u'show', u'doing', u'is', u'as', u'skate', u'pulling', u'table', u'stage', u'top', u'riding', u'skateboarder', u'his', u'watching', u'putting', u'using', u'with', u'man', u'a', u'on', u'of', u'boarder', u'skateboard', u'trick', u'person', u'the', u'tricks'])\n"
     ]
    }
   ],
   "source": [
    "word_set = set()\n",
    "for i in np.arange(0, int(len(data)/2)) :\n",
    "    img, captions = data[i]\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "        [word_set.add(word) for word in caption.replace(\".\",\"\").lower().split()]\n",
    "    break;\n",
    "print(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'dentist'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indx_to_word_dict['9486']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9487"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indx_to_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "15565\n"
     ]
    }
   ],
   "source": [
    "word_set = set()\n",
    "word_set.add('unk_word')\n",
    "for i in np.arange(0, int(len(data)/2)) :\n",
    "    img, captions = data[i]\n",
    "    for caption in captions:\n",
    "        [word_set.add(word) for word in caption.replace(\".\",\"\").lower().split()]\n",
    "    \n",
    "    if (i%1000 == 0):\n",
    "        print(\"DOne 100\")\n",
    "print(len(word_set))\n",
    "srted_word_set = sorted(word_set)\n",
    "\n",
    "word_to_indx_dict = {srted_word_set[i]:i for i in np.arange(0, len(srted_word_set))}\n",
    "indx_to_word_dict = {i:srted_word_set[i] for i in np.arange(0, len(srted_word_set))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a man in glasses is holding a wine glass', 'A man holds a glass in aroom with many other people', 'A man holds a glass as others mill around behind him. ', 'A group of people standing around in a room.', 'A man holding a glass speaking to someone']\n"
     ]
    }
   ],
   "source": [
    "char_set = set()\n",
    "word_set = set()\n",
    "for i in np.arange(0, trainY.shape[0]):\n",
    "    [char_set.add(c) for c in trainY[i]]\n",
    "    [word_set.add(word) for word in trainY[i].split(' ')]\n",
    "char_set.add('unk_char')\n",
    "char_set.add('sos')\n",
    "char_set.add('eos')\n",
    "srted_char_set = sorted(char_set)\n",
    "word_set.add('unk_word')\n",
    "word_set.add('sos')\n",
    "word_set.add('eos')\n",
    "srted_word_set = sorted(word_set)\n",
    "char_to_indx_dict = {srted_char_set[i]:i for i in np.arange(0, len(srted_char_set))}\n",
    "indx_to_char_dict = {i:srted_char_set[i] for i in np.arange(0, len(srted_char_set))}\n",
    "\n",
    "word_to_indx_dict = {srted_word_set[i]:i for i in np.arange(0, len(srted_word_set))}\n",
    "indx_to_word_dict = {i:srted_word_set[i] for i in np.arange(0, len(srted_word_set))}\n",
    "\n",
    "\n",
    "trainY_proc_data_c = []\n",
    "trainY_proc_data_w = []\n",
    "for i in np.arange(0, trainY.shape[0]):\n",
    "    curr_numpy_c = [char_to_indx_dict[c] for c in trainY[i]]\n",
    "    curr_numpy_w = [word_to_indx_dict[word] for word in trainY[i].split(' ')]\n",
    "    trainY_proc_data_c.append(np.array( [char_to_indx_dict['sos']] + curr_numpy_c + [char_to_indx_dict['eos']]))\n",
    "    trainY_proc_data_w.append(np.array([word_to_indx_dict['sos']]+ curr_numpy_w +[word_to_indx_dict['eos']]))\n",
    "print(type(np.array(trainY_proc_data_c)))\n",
    "print(type(np.array(trainY_proc_data_w)))\n",
    "\n",
    "np.save('trainY_proc_data_c.npy', trainY_proc_data_c)\n",
    "np.save('trainY_proc_data_w.npy', trainY_proc_data_w)\n",
    "\n",
    "\n",
    "    replace(\".\",\"\").lower().split()\n",
    "\n",
    "\n",
    "img, captions = cap[40503]\n",
    "len(cap[40503])\n",
    "print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([u'14', u'881', u'35', u'32', u'818', u'189', u'114', u'14', u'606',\n",
      "       u'364'], dtype='<U3')\n",
      " array([u'1', u'947', u'881', u'35', u'119', u'1', u'4', u'606', u'627'],\n",
      "      dtype='<U3')\n",
      " array([u'14', u'1121', u'881', u'35', u'122', u'61', u'23', u'14', u'606',\n",
      "       u'1061'], dtype='<U4')\n",
      " array([u'1', u'881', u'6', u'1', u'2436', u'414', u'50', u'35', u'452',\n",
      "       u'947'], dtype='<U4')\n",
      " array([u'1', u'881', u'352', u'119', u'1', u'606', u'627'], dtype='<U3')]\n"
     ]
    }
   ],
   "source": [
    "devY_proc_data_w = []\n",
    "for i in np.arange(0, int(len(data)/2)):\n",
    "    img, captions = data[i]\n",
    "    curr_image_captions = []\n",
    "    for caption in captions:\n",
    "        words_curr = caption.replace(\".\",\"\").replace(\",\",\"\").lower().split()\n",
    "        caps_ind = []\n",
    "        for word in words_curr:\n",
    "            if word in word_to_indx_dict.keys():\n",
    "                caps_ind.append(int(word_to_indx_dict[word]))\n",
    "            else:\n",
    "                continue;\n",
    "        #print(len(caps_ind))       \n",
    "        #caps_ind = [word_to_indx_dict[word] for word in caption.replace(\".\",\"\").replace(\",\",\"\")lower().split()]\n",
    "        curr_image_captions.append(np.array(caps_ind))\n",
    "    devY_proc_data_w.append(np.array(curr_image_captions))\n",
    "print(devY_proc_data_w[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('trainX.npy', trainY_proc_data_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-96843709450c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_indx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'eos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "int(word_to_indx_dict[u'eos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
