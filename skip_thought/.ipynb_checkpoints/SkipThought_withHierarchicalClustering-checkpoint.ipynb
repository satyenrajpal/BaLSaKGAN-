{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import _use_shared_memory\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from collections import namedtuple\n",
    "import torch.nn.utils.weight_norm as weightNorm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionLoader():\n",
    "    \"\"\" Load the WSJ speech dataset with transcripts.\n",
    "        \n",
    "        Ensure WSJ_PATH is path to directory containing \n",
    "        all data files (.npy) provided on Kaggle.\n",
    "        \n",
    "        Example usage:\n",
    "            loader = WSJ()\n",
    "            trainX, trainY = loader.train\n",
    "            assert(trainX.shape[0] == 24590)\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    os.environ['WSJ_PATH'] = \"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dev_set = None\n",
    "        self.train_set = None\n",
    "        self.test_set = None\n",
    "  \n",
    "    @property\n",
    "    def dev(self):\n",
    "        if self.dev_set is None:\n",
    "            self.dev_set = load_raw(os.environ['WSJ_PATH'], 'dev')\n",
    "        return self.dev_set\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        if self.train_set is None:\n",
    "            self.train_set = load_raw(os.environ['WSJ_PATH'], 'train')\n",
    "        return self.train_set\n",
    "  \n",
    "    @property\n",
    "    def test(self):\n",
    "        if self.test_set is None:\n",
    "            self.test_set = (np.load(os.path.join(os.environ['WSJ_PATH'], 'test.npy'), encoding='bytes'), None)\n",
    "        return self.test_set\n",
    "    \n",
    "def load_raw(path, name):\n",
    "    return (\n",
    "        np.load(os.path.join(path, 'trainX.npy'.format(name)), encoding='bytes'), \n",
    "        np.load(os.path.join(path, 'trainX.npy'.format(name)), encoding='bytes')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "captionload = CaptionLoader()\n",
    "trainX, trainX = captionload.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, X, Y=None):\n",
    "        self.X = X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cap = np.random.randint(0,len(self.X[index]))\n",
    "        return self.X[index][(cap-1)%len(self.X[index])], self.X[index][cap], self.X[index][(cap+1)%len(self.X[index])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCaptrain = CaptionDataset(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \n",
    "    ## Sorting based on the utternce lenght\n",
    "    # middle sentence is the one required to send into the encoder to produce the thoughts\n",
    "    data.sort(key=lambda x: x[1].shape[0], reverse=True)\n",
    "    \n",
    "    ## Unzip the data\n",
    "    X, Y, Z = zip(*data)\n",
    "    \n",
    "    ## Now padding the X\n",
    "    Xlenghts = [Sample.shape[0] for Sample in X]\n",
    "    Ylenghts = [Sample.shape[0] for Sample in Y]\n",
    "    Zlenghts = [Sample.shape[0] for Sample in Z]\n",
    "    \n",
    "    if _use_shared_memory:\n",
    "        paddedArrayX = torch.FloatStorage._new_shared(max(Xlenghts)*len(Xlenghts)).new(max(Xlenghts),len(Xlenghts)).zero_()\n",
    "        maskArrayX = torch.FloatStorage._new_shared(max(Xlenghts)*len(Xlenghts)).new(max(Xlenghts),len(Xlenghts)).zero_()\n",
    "        \n",
    "        paddedArrayY = torch.FloatStorage._new_shared(max(Ylenghts)*len(Ylenghts)).new(max(Ylenghts),len(Ylenghts)).zero_()\n",
    "        \n",
    "        paddedArrayZ = torch.FloatStorage._new_shared(max(Zlenghts)*len(Zlenghts)).new(max(Zlenghts),len(Zlenghts)).zero_()\n",
    "        maskArrayZ = torch.FloatStorage._new_shared(max(Zlenghts)*len(Zlenghts)).new(max(Zlenghts),len(Zlenghts)).zero_()\n",
    "    else:\n",
    "        paddedArrayX = torch.FloatTensor(max(Xlenghts),len(Xlenghts)).zero_()\n",
    "        maskArrayX = torch.FloatTensor(max(Xlenghts),len(Xlenghts)).zero_()\n",
    "        \n",
    "        paddedArrayY = torch.FloatTensor(max(Ylenghts),len(Ylenghts)).zero_()\n",
    "        \n",
    "        paddedArrayZ = torch.FloatTensor(max(Zlenghts),len(Zlenghts)).zero_()\n",
    "        maskArrayZ = torch.FloatTensor(max(Zlenghts),len(Zlenghts)).zero_()\n",
    "        \n",
    "    for idx, Sample in enumerate(X):\n",
    "        paddedArrayX[:Sample.shape[0], idx] = torch.from_numpy(Sample)\n",
    "        maskArrayX[:Sample.shape[0], idx] = 1\n",
    "        \n",
    "    for idx, Sample in enumerate(Y):\n",
    "        paddedArrayY[:Sample.shape[0], idx] = torch.from_numpy(Sample)\n",
    "        \n",
    "    for idx, Sample in enumerate(Z):\n",
    "        paddedArrayZ[:Sample.shape[0], idx] = torch.from_numpy(Sample)\n",
    "        maskArrayZ[:Sample.shape[0], idx] = 1\n",
    "\n",
    "    return paddedArrayX, maskArrayX, paddedArrayY, Ylenghts, paddedArrayZ, maskArrayZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLoader = DataLoader(dataCaptrain,\n",
    "                          batch_size=32,\n",
    "                          collate_fn=collate_fn,\n",
    "                          shuffle=True,\n",
    "                          #num_workers = 3,\n",
    "                          #pin_memory=True # CUDA only\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch_idx, (paddedArrayX, Xlenghts, paddedArrayY, maskArrayY, paddedArrayZ, maskArrayZ) in enumerate(TrainLoader):\n",
    "#    print(batch_idx)\n",
    "#    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STEncoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = 2\n",
    "        self.thought_size = 128\n",
    "        self.direction = 2\n",
    "        self.embedding_dim = 128\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.thought_size, num_layers=self.num_layers, batch_first=False, bidirectional=True)\n",
    "\n",
    "        self.keys = nn.Linear(in_features=self.direction*self.thought_size, out_features=128)\n",
    "        self.values = nn.Linear(in_features=self.direction*self.thought_size, out_features=128)\n",
    "        \n",
    "    def hidden_init(self, batch_size):      \n",
    "        if torch.cuda.is_available():\n",
    "            return (autograd.Variable(torch.zeros(self.direction*self.num_layers, batch_size, self.thought_size).cuda()),\n",
    "                    autograd.Variable(torch.zeros(self.direction*self.num_layers, batch_size, self.thought_size).cuda()))\n",
    "        return (autograd.Variable(torch.zeros(self.direction*self.num_layers, batch_size, self.thought_size)),\n",
    "                autograd.Variable(torch.zeros(self.direction*self.num_layers, batch_size, self.thought_size))) \n",
    "        \n",
    "    def forward(self, input, inputLenghts):\n",
    "        packedInputX = pack_padded_sequence(input, inputLenghts, batch_first = False)\n",
    "        inputX, self.hidden  = self.rnn(packedInputX)\n",
    "        padOutputX, _ = pad_packed_sequence(inputX)\n",
    "       \n",
    "        output = padOutputX.permute(1, 0, 2)\n",
    "        \n",
    "        ## Output would be <batch_size> x <sequence_len> x 128\n",
    "        keys = self.keys(output)\n",
    "        values = self.values(output)\n",
    "        \n",
    "        return keys, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDuoDecoderAttn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STDuoDecoderAttn, self).__init__()\n",
    "        self.hidden_size = 256\n",
    "        self.embedding_dim = 128\n",
    "        self.num_embeddings = 15565 ## give current ones \n",
    "        self.key_space = 128\n",
    "        \n",
    "        #Hidden units\n",
    "        hidden1 = torch.zeros(1, self.hidden_size)\n",
    "        hidden2 = torch.zeros(1, self.hidden_size)\n",
    "        if torch.cuda.is_available():\n",
    "            hidden1 = hidden1.cuda()\n",
    "            hidden2 = hidden2.cuda()\n",
    "        \n",
    "        self.hidden1 = nn.Parameter(hidden1)\n",
    "        self.hidden2 = nn.Parameter(hidden2)\n",
    "        \n",
    "        #embedding before RNN\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.num_embeddings, embedding_dim=self.embedding_dim)\n",
    "        \n",
    "        #MLP layer for conetext to project into key space\n",
    "        self.keySpaceProjection = nn.Linear(in_features=self.hidden_size, out_features=self.key_space)\n",
    "        \n",
    "        ##LSTM cells for the decoder\n",
    "        self.lstmcell_prev = nn.LSTMCell(self.key_space+self.embedding_dim, self.hidden_size)\n",
    "        self.lstmcell_next = nn.LSTMCell(self.key_space+self.embedding_dim, self.hidden_size)\n",
    "        \n",
    "        self.wordProject = nn.Linear(in_features=self.hidden_size, out_features=self.num_embeddings)\n",
    "        \n",
    "    def forward(self, inputPrev, inputNext, keys, values):\n",
    "        hidden10 = self.hidden1.expand(inputPrev.size()[1], -1).contiguous()\n",
    "        hidden20 = self.hidden2.expand(inputNext.size()[1], -1).contiguous()\n",
    "        \n",
    "        c10 = Variable(torch.zeros(inputPrev.size(1), self.hidden_size), requires_grad=False)\n",
    "        c20 = Variable(torch.zeros(inputNext.size(1), self.hidden_size), requires_grad=False)        \n",
    "        if torch.cuda.is_available():\n",
    "            c10 = Variable(torch.zeros(inputPrev.size(1), self.hidden_size).cuda(), requires_grad=False)\n",
    "            c20 = Variable(torch.zeros(inputNext.size(1), self.hidden_size).cuda(), requires_grad=False)\n",
    "        \n",
    "        ## geting the context for first time from hidden unit\n",
    "        context_prev = self.attentionQuery(hidden10, keys, values)\n",
    "        \n",
    "        logits_prev = []\n",
    "        ##concatenate the context and embedding[0]\n",
    "        for i in np.arange(0, inputPrev.size()[0]):\n",
    "            output_curr = torch.cat((inputPrev[i], context_prev), 1)\n",
    "            hidden10, _ = self.lstmcell_prev(output_curr, (hidden10, c10))\n",
    "            \n",
    "            projection_out = self.wordProject(hidden10)\n",
    "            logits_prev.append(projection_out)\n",
    "            context_prev = self.attentionQuery(hidden10, keys, values)\n",
    "            \n",
    "            ## Project layer\n",
    "        logits_prev = torch.stack(logits_prev)\n",
    "        \n",
    "        ## geting the context for first time from hidden unit\n",
    "        context_prev = self.attentionQuery(hidden20, keys, values)\n",
    "        \n",
    "        logits_next = []\n",
    "        ##concatenate the context and embedding[0]\n",
    "        for i in np.arange(0, inputNext.size()[0]):\n",
    "            output_curr = torch.cat((inputNext[i], context_prev), 1)\n",
    "            hidden20, _ = self.lstmcell_next(output_curr, (hidden20, c20))\n",
    "            \n",
    "            projection_out = self.wordProject(hidden20)\n",
    "            logits_next.append(projection_out)\n",
    "            context_prev = self.attentionQuery(hidden20, keys, values)\n",
    "            \n",
    "            ## Project layer\n",
    "        logits_next = torch.stack(logits_next)        \n",
    "\n",
    "        return logits_prev, logits_next\n",
    "        \n",
    "    def attentionQuery(self, input, keys, values):\n",
    "        ##linear layer to project into key space\n",
    "        output = self.keySpaceProjection(input)\n",
    "        \n",
    "        keys = keys.permute(0, 2, 1)\n",
    "        ##performing bmm \n",
    "        output = torch.bmm(output.unsqueeze(1), keys)\n",
    "        \n",
    "        ##peform softmax\n",
    "        output = F.softmax(output, dim=2)\n",
    "        \n",
    "        ##perform bmm to get the context\n",
    "        output = torch.bmm(output, values)\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniSKIP_variant(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(UniSKIP_variant, self).__init__()\n",
    "        self.encoder = STEncoder()\n",
    "        self.decoder = STDuoDecoderAttn()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = 128\n",
    "        \n",
    "        self.wordembed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n",
    "        \n",
    "    def forward(self, inputCurr, inputLenghts, inputPrev, inputNext):\n",
    "        \n",
    "        # Get the thought of the current sentence\n",
    "        word_embed_curr = F.tanh(self.wordembed(inputCurr))\n",
    "        keys, values = self.encoder(word_embed_curr, inputLenghts)\n",
    "        \n",
    "        # Get the embedding for prev and next sentence \n",
    "        word_embed_prev = F.tanh(self.wordembed(inputPrev))        \n",
    "        word_embed_next = F.tanh(self.wordembed(inputNext))\n",
    "        \n",
    "        logits_prev, logits_next = self.decoder(word_embed_prev, word_embed_next, keys, values)\n",
    "        \n",
    "        return logits_prev, logits_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, loader, loss):\n",
    "    i = 0\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (paddedArray, Xlenghts, Yarray, Ylenghts) in enumerate(loader):\n",
    "        model.hidden = model.hidden_init(paddedArray.size()[1])\n",
    "        if torch.cuda.is_available():\n",
    "            paddedArray = paddedArray.cuda()\n",
    "                    \n",
    "        X = Variable(paddedArray)\n",
    "        Y = Yarray.int()\n",
    "        out = model(X, Xlenghts)\n",
    "                \n",
    "        act_lens = torch.from_numpy(np.asarray(Xlenghts)).int()\n",
    "        label_lens = torch.from_numpy(np.asarray(Ylenghts)).int()\n",
    "        loss_val = loss(out, Variable(Y+1), Variable(act_lens), Variable(label_lens))\n",
    "        epoch_loss += loss_val.data[0]\n",
    "        \n",
    "        if (i%5 == 0):\n",
    "            print(loss_val.data[0])\n",
    "        i = i + 1\n",
    "            \n",
    "    print(\"Validation Loss\")\n",
    "    print(epoch_loss/(i*32))\n",
    "    return\n",
    "\n",
    "class Trainer():\n",
    "    \"\"\" A simple training cradle\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, train_loader, load_path=None):\n",
    "        self.model = model\n",
    "        if load_path is not None:\n",
    "            self.model = torch.load(load_path)\n",
    "        self.optimizer = optimizer\n",
    "        #self.loss = CrossEntropyLoss3D()\n",
    "        self.loss = nn.CrossEntropyLoss().cuda()\n",
    "        self.loader = train_loader\n",
    "        \n",
    "    def stop_cond(self):\n",
    "        # TODO: Implement early stopping\n",
    "        def deriv(ns):\n",
    "            return [ns[i+1] - ns[i] for i in range(len(ns)-1)]\n",
    "        val_errors = [m.val_error for m in self.metrics]\n",
    "        back = val_errors[-138:]\n",
    "        return sum(deriv(back)) > 0\n",
    "            \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def run(self, n_epochs):\n",
    "        print(\"begin training...\")\n",
    "        self.metrics = []\n",
    "        for e in range(n_epochs):\n",
    "            if self.stop_cond():\n",
    "                return\n",
    "            epoch_loss = 0\n",
    "            correct = 0\n",
    "            i = 0\n",
    "            self.optimizer.zero_grad()\n",
    "            for batch_idx, (paddedArrayPrev, maskArrayPrev, paddedArrayCurr, Currlenghts, paddedArrayNext, maskArrayNext) in enumerate(self.loader):\n",
    "                self.model.encoder.hidden = self.model.encoder.hidden_init(paddedArrayCurr.size()[1])\n",
    "                if torch.cuda.is_available():\n",
    "                    paddedArrayCurr = paddedArrayCurr.cuda()\n",
    "                    \n",
    "                Curr_Sen = Variable(paddedArrayCurr.type(torch.LongTensor))\n",
    "\n",
    "                #Y = Yarray.int()\n",
    "                logits_prev, logits_next = self.model(Curr_Sen, Currlenghts, Variable(paddedArrayPrev[:-1,:].type(torch.LongTensor)), Variable(paddedArrayNext[:-1,:].type(torch.LongTensor)))\n",
    "\n",
    "                logits_prev = logits_prev.contiguous().view(-1, logits_prev.size()[2])\n",
    "                logits_next = logits_next.contiguous().view(-1, logits_next.size()[2])\n",
    "                \n",
    "                Y_prev = paddedArrayPrev[1:,:]\n",
    "                Y_prev = Y_prev.contiguous().view(-1)\n",
    "\n",
    "                Y_next = paddedArrayNext[1:,:]\n",
    "                Y_next = Y_next.contiguous().view(-1)\n",
    "\n",
    "                maskArrayPrev = maskArrayPrev[1:,:]\n",
    "                maskArrayPrev = maskArrayPrev.contiguous().view(-1)\n",
    "\n",
    "                maskArrayNext = maskArrayNext[1:,:]\n",
    "                maskArrayNext = maskArrayNext.contiguous().view(-1)\n",
    "    \n",
    "                ind_prev = torch.nonzero(maskArrayPrev, out=None).squeeze()\n",
    "                ind_next = torch.nonzero(maskArrayNext, out=None).squeeze()\n",
    "                \n",
    "                valid_target_prev = torch.index_select(Y_prev, 0, ind_prev).type(torch.LongTensor)\n",
    "                valid_output_prev = torch.index_select(logits_prev, 0, Variable(ind_prev))\n",
    "\n",
    "                valid_target_next = torch.index_select(Y_next, 0, ind_next).type(torch.LongTensor)\n",
    "                valid_output_next = torch.index_select(logits_next, 0, Variable(ind_next))\n",
    "\n",
    "                loss_prev = self.loss(valid_output_prev, Variable(valid_target_prev))\n",
    "                loss_next = self.loss(valid_output_next, Variable(valid_target_next))\n",
    "                \n",
    "                loss = loss_prev + loss_next\n",
    "                loss.backward()     \n",
    "                   \n",
    "                nn.utils.clip_grad_norm(self.model.parameters(), 0.25)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                epoch_loss += loss.data[0]\n",
    "                \n",
    "                if(i%20 == 0):\n",
    "                    print(loss.data[0])\n",
    "                i = i + 1\n",
    "            \n",
    "            #if (e%2 == 0):\n",
    "            #    inference(self.model, self.valLoader, self.loss)\n",
    "            print(\"Training Loss\")\n",
    "            print(epoch_loss/(i))\n",
    "            print(\"DONE WITH ONE EPOCH\")\n",
    "            print(e)\n",
    "            wikiTrainer.save_model('./SKIP_THOUGHT_MODEL.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWiki = UniSKIP_variant(15565) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training...\n",
      "19.316110610961914\n",
      "12.833284378051758\n",
      "11.92369270324707\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f4e6b06bb8fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelWiki\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mwikiTrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelWiki\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mwikiTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-6c6359d14b9a>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, n_epochs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_prev\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_next\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env_new\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env_new\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    print(\"GPU available\") \n",
    "    modelWiki.cuda()\n",
    "\n",
    "#modelWiki.load_state_dict(torch.load('./SKIP_THOUGHT_MODEL.pt'))\n",
    "#optimizer = torch.optim.SGD(modelWiki.parameters(), lr=0.1, momentum=0.9) \n",
    "optimizer = torch.optim.Adam(modelWiki.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "wikiTrainer = Trainer(modelWiki, optimizer, TrainLoader) \n",
    "wikiTrainer.run(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
