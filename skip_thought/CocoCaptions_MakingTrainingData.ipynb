{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.76s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "data = dset.CocoCaptions(root = './val2014/val2014/',\n",
    "                        annFile = './annotations_trainval2014/annotations/captions_val2014.json',\n",
    "                        transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "DOne 100\n",
      "15565\n"
     ]
    }
   ],
   "source": [
    "word_set = set()\n",
    "word_set.add('unk_word')\n",
    "for i in np.arange(0, int(len(data)/2)) :\n",
    "    img, captions = data[i]\n",
    "    for caption in captions:\n",
    "        [word_set.add(word) for word in caption.replace(\".\",\"\").lower().split()]\n",
    "    \n",
    "    if (i%1000 == 0):\n",
    "        print(\"DOne 100\")\n",
    "print(len(word_set))\n",
    "srted_word_set = sorted(word_set)\n",
    "\n",
    "word_to_indx_dict = {srted_word_set[i]:i for i in np.arange(0, len(srted_word_set))}\n",
    "indx_to_word_dict = {i:srted_word_set[i] for i in np.arange(0, len(srted_word_set))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a man in glasses is holding a wine glass', 'A man holds a glass in aroom with many other people', 'A man holds a glass as others mill around behind him. ', 'A group of people standing around in a room.', 'A man holding a glass speaking to someone']\n"
     ]
    }
   ],
   "source": [
    "char_set = set()\n",
    "word_set = set()\n",
    "for i in np.arange(0, trainY.shape[0]):\n",
    "    [char_set.add(c) for c in trainY[i]]\n",
    "    [word_set.add(word) for word in trainY[i].split(' ')]\n",
    "char_set.add('unk_char')\n",
    "char_set.add('sos')\n",
    "char_set.add('eos')\n",
    "srted_char_set = sorted(char_set)\n",
    "word_set.add('unk_word')\n",
    "word_set.add('sos')\n",
    "word_set.add('eos')\n",
    "srted_word_set = sorted(word_set)\n",
    "char_to_indx_dict = {srted_char_set[i]:i for i in np.arange(0, len(srted_char_set))}\n",
    "indx_to_char_dict = {i:srted_char_set[i] for i in np.arange(0, len(srted_char_set))}\n",
    "\n",
    "word_to_indx_dict = {srted_word_set[i]:i for i in np.arange(0, len(srted_word_set))}\n",
    "indx_to_word_dict = {i:srted_word_set[i] for i in np.arange(0, len(srted_word_set))}\n",
    "\n",
    "\n",
    "trainY_proc_data_c = []\n",
    "trainY_proc_data_w = []\n",
    "for i in np.arange(0, trainY.shape[0]):\n",
    "    curr_numpy_c = [char_to_indx_dict[c] for c in trainY[i]]\n",
    "    curr_numpy_w = [word_to_indx_dict[word] for word in trainY[i].split(' ')]\n",
    "    trainY_proc_data_c.append(np.array( [char_to_indx_dict['sos']] + curr_numpy_c + [char_to_indx_dict['eos']]))\n",
    "    trainY_proc_data_w.append(np.array([word_to_indx_dict['sos']]+ curr_numpy_w +[word_to_indx_dict['eos']]))\n",
    "print(type(np.array(trainY_proc_data_c)))\n",
    "print(type(np.array(trainY_proc_data_w)))\n",
    "\n",
    "np.save('trainY_proc_data_c.npy', trainY_proc_data_c)\n",
    "np.save('trainY_proc_data_w.npy', trainY_proc_data_w)\n",
    "\n",
    "\n",
    "    replace(\".\",\"\").lower().split()\n",
    "\n",
    "\n",
    "img, captions = cap[40503]\n",
    "len(cap[40503])\n",
    "print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  393, 13497, 15318,   393,  6767,  5905, 14711,   720,  4998,\n",
      "        5382,  3119,  9341,  9125,  7144])\n",
      " array([  393, 10181,  6919,   393, 14711, 12235,   986, 13774,  4734,\n",
      "        9125,   393, 13497])\n",
      " array([  393, 14711, 15318,  5382,  6919,  7144, 15318,  7911, 13021,\n",
      "       12240,  9199,   393, 13497, 15318,  2357])\n",
      " array([ 5382,  6919,   393,  2908, 14711, 12240,  9199,   393, 13497])\n",
      " array([  393,  7582,  2602, 13769,  7130, 12240,  9199, 13774,  4602,\n",
      "        9125,   393,  4125, 13497])]\n"
     ]
    }
   ],
   "source": [
    "trainY_proc_data_w = []\n",
    "for i in np.arange(0, int(len(data)/2)):\n",
    "    img, captions = data[i]\n",
    "    curr_image_captions = []\n",
    "    for caption in captions:\n",
    "        caps_ind = [word_to_indx_dict[word] for word in caption.replace(\".\",\"\").lower().split()]\n",
    "        curr_image_captions.append(np.array(caps_ind))\n",
    "    trainY_proc_data_w.append(np.array(curr_image_captions))\n",
    "print(trainY_proc_data_w[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('trainX.npy', trainY_proc_data_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
